#include "http_server.h"
#include <iostream>
#include <signal.h>
#include <memory>
#include <chrono>
#include <thread>
#include <iomanip>

// Global server instance for signal handling
std::unique_ptr<HttpServer> g_server;

void signalHandler(int signal) {
    std::cout << "\nðŸ›‘ Received signal " << signal << ". Shutting down gracefully..." << std::endl;
    
    if (g_server) {
        g_server->stop();
    }
    
    exit(0);
}

void printBanner() {
    std::cout << R"(
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•    â•šâ•â•  â•šâ•â•â•šâ•â•
                                                   
âœ¨ AI-Powered Quiz & Psychology Analysis Platform
ðŸ§  Real AI Generation with DistilGPT-2 + llama.cpp
âš¡ High-Performance C++ Implementation
ðŸ”® Advanced Personality Analysis & Quiz Generation

)" << std::endl;
}

void printServerInfo(const std::string& host, int port, const std::string& modelPath) {
    std::cout << "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”" << std::endl;
    std::cout << "â”‚               AEON AI SERVER INFO               â”‚" << std::endl;
    std::cout << "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤" << std::endl;
    std::cout << "â”‚ Host: " << std::setw(36) << std::left << host << " â”‚" << std::endl;
    std::cout << "â”‚ Port: " << std::setw(36) << std::left << port << " â”‚" << std::endl;
    std::cout << "â”‚ API Base: http://" << host << ":" << port << std::setw(16) << " â”‚" << std::endl;
    std::cout << "â”‚ Model: DistilGPT-2 (GGUF)" << std::setw(17) << " â”‚" << std::endl;
    std::cout << "â”‚ Model Path: " << std::setw(29) << std::left << modelPath.substr(0, 29) << " â”‚" << std::endl;
    std::cout << "â”‚ Backend: llama.cpp (CPU optimized)" << std::setw(9) << " â”‚" << std::endl;
    std::cout << "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜" << std::endl;
}

void printEndpoints() {
    std::cout << "\nðŸ”Œ Available API Endpoints:" << std::endl;
    std::cout << "â”œâ”€ GET  /                       â†’ Status & health check" << std::endl;
    std::cout << "â”‚" << std::endl;
    std::cout << "â”œâ”€ Quiz Generation:" << std::endl;
    std::cout << "â”‚  â”œâ”€ POST /api/quiz/generate   â†’ Generate quiz question" << std::endl;
    std::cout << "â”‚  â””â”€ GET  /api/quiz/categories â†’ List available categories" << std::endl;
    std::cout << "â”‚" << std::endl;
    std::cout << "â”œâ”€ Psychology Analysis:" << std::endl;
    std::cout << "â”‚  â”œâ”€ GET  /api/psychology/traits   â†’ Get personality traits" << std::endl;
    std::cout << "â”‚  â”œâ”€ POST /api/psychology/generate â†’ Create psychology questions" << std::endl;
    std::cout << "â”‚  â””â”€ POST /api/psychology/analyze  â†’ Analyze personality profile" << std::endl;
    std::cout << "â”‚" << std::endl;
    std::cout << "â””â”€ System Information:" << std::endl;
    std::cout << "   â”œâ”€ GET  /api/stats         â†’ Detailed server statistics" << std::endl;
    std::cout << "   â””â”€ GET  /api/model/info    â†’ AI model information\n" << std::endl;
}

void printAIFeatures() {
    std::cout << "âœ¨ AEON AI Core Features:" << std::endl;
    std::cout << "â”œâ”€ Real-time AI Generation    â†’ Each response is unique" << std::endl;
    std::cout << "â”œâ”€ Dynamic Content Creation   â†’ No pre-written content" << std::endl;
    std::cout << "â”œâ”€ Psychology Assessment      â†’ MBTI & personality analysis" << std::endl;
    std::cout << "â”œâ”€ Smart Context Processing   â†’ Adaptive question generation" << std::endl;
    std::cout << "â”œâ”€ Robust Error Recovery      â†’ Graceful fallback systems" << std::endl;
    std::cout << "â””â”€ Comprehensive Analytics    â†’ Performance monitoring\n" << std::endl;
}

void printPerformanceInfo() {
    std::cout << "âš¡ Technical Architecture:" << std::endl;
    std::cout << "â”œâ”€ Engine: llama.cpp          â†’ High-performance inference" << std::endl;
    std::cout << "â”œâ”€ Optimization: AVX2/AVX512  â†’ CPU vectorization" << std::endl;
    std::cout << "â”œâ”€ Storage: Quantized Models  â†’ Efficient GGUF format (Q4_K_M)" << std::endl;
    std::cout << "â”œâ”€ Processing: Multi-threaded â†’ Concurrent request handling" << std::endl;
    std::cout << "â”œâ”€ Memory: Smart Management   â†’ Optimized token context" << std::endl;
    std::cout << "â””â”€ Networking: Async I/O      â†’ High-throughput HTTP server\n" << std::endl;
}

void printUsageExamples(const std::string& host, int port) {
    std::cout << "ðŸ“˜ API Usage Examples:" << std::endl;
    std::cout << "â”Œâ”€ Server Status:" << std::endl;
    std::cout << "â”‚  curl http://" << host << ":" << port << "/" << std::endl;
    std::cout << "â”‚" << std::endl;
    std::cout << "â”œâ”€ Quiz Generation API:" << std::endl;
    std::cout << "â”‚  curl -X POST http://" << host << ":" << port << "/api/quiz/generate \\" << std::endl;
    std::cout << "â”‚    -H \"Content-Type: application/json\" \\" << std::endl;
    std::cout << "â”‚    -d '{" << std::endl;
    std::cout << "â”‚      \"category\": \"Science\"," << std::endl;
    std::cout << "â”‚      \"difficulty\": \"Medium\"," << std::endl;
    std::cout << "â”‚      \"playerName\": \"User\"" << std::endl;
    std::cout << "â”‚    }'" << std::endl;
    std::cout << "â”‚" << std::endl;
    std::cout << "â”œâ”€ Psychology Analysis API:" << std::endl;
    std::cout << "â”‚  curl -X POST http://" << host << ":" << port << "/api/psychology/analyze \\" << std::endl;
    std::cout << "â”‚    -H \"Content-Type: application/json\" \\" << std::endl;
    std::cout << "â”‚    -d '{" << std::endl;
    std::cout << "â”‚      \"answers\": [" << std::endl;
    std::cout << "â”‚        {\"questionId\": 1, \"selectedOption\": 0, \"trait\": \"E/I\"}," << std::endl;
    std::cout << "â”‚        {\"questionId\": 2, \"selectedOption\": 1, \"trait\": \"S/N\"}" << std::endl;
    std::cout << "â”‚      ]" << std::endl;
    std::cout << "â”‚    }'" << std::endl;
    std::cout << "â”‚" << std::endl;
    std::cout << "â””â”€ System Information:" << std::endl;
    std::cout << "   curl http://" << host << ":" << port << "/api/stats" << std::endl;
    std::cout << std::endl;
}

void printExpectedPerformance() {
    std::cout << "ðŸš€ System Requirements & Performance:" << std::endl;
    std::cout << "â”œâ”€ Recommended:      4+ CPU cores, 8GB+ RAM" << std::endl;
    std::cout << "â”œâ”€ Memory Usage:     1.5-2.5GB (models + runtime)" << std::endl;
    std::cout << "â”œâ”€ Response Time:    1-3 seconds per AI operation" << std::endl;
    std::cout << "â”œâ”€ Concurrency:      15-25 simultaneous users" << std::endl;
    std::cout << "â”œâ”€ Startup Time:     5-15 seconds (model loading)" << std::endl;
    std::cout << "â””â”€ Throughput:       1000-3000 operations/hour\n" << std::endl;
}

int main(int argc, char* argv[]) {
    // Print banner
    printBanner();
    
    // Parse command line arguments
    std::string host = "0.0.0.0";
    int port = 8080;
    std::string modelPath = "models/distilgpt2.Q4_K_M.gguf";
    
    for (int i = 1; i < argc; i++) {
        std::string arg = argv[i];
        if ((arg == "--host" || arg == "-h") && i + 1 < argc) {
            host = argv[++i];
        } else if ((arg == "--port" || arg == "-p") && i + 1 < argc) {
            port = std::stoi(argv[++i]);
        } else if ((arg == "--model" || arg == "-m") && i + 1 < argc) {
            modelPath = argv[++i];
        } else if (arg == "--help") {
            std::cout << "Usage: " << argv[0] << " [options]" << std::endl;
            std::cout << "Options:" << std::endl;
            std::cout << "  --host, -h <host>     Server host (default: 0.0.0.0)" << std::endl;
            std::cout << "  --port, -p <port>     Server port (default: 8080)" << std::endl;
            std::cout << "  --model, -m <path>    Model path (default: models/distilgpt2.Q4_K_M.gguf)" << std::endl;
            std::cout << "  --help                Show this help message" << std::endl;
            return 0;
        }
    }
    
    // Print server information
    printServerInfo(host, port, modelPath);
    printEndpoints();
    printAIFeatures();
    printPerformanceInfo();
    printUsageExamples(host, port);
    printExpectedPerformance();
    
    try {
        std::cout << "ðŸ”„ Initializing AEON AI Server..." << std::endl;
        
        // Create server instance with AI model
        g_server = std::make_unique<HttpServer>(host, port, modelPath);
        
        // Set up signal handlers for graceful shutdown
        signal(SIGINT, signalHandler);   // Ctrl+C
        signal(SIGTERM, signalHandler);  // Termination signal
        
        std::cout << "âœ… Server core initialized successfully!" << std::endl;
        
        if (g_server->isAIModelLoaded()) {
            std::cout << "ðŸ§  Neural models loaded and ready for inference!" << std::endl;
        } else {
            std::cout << "âš ï¸ Warning: AI models failed to load - check model path!" << std::endl;
        }
        
        std::cout << "ðŸš€ Starting HTTP API service..." << std::endl;
        
        // Start the server
        if (!g_server->start()) {
            std::cerr << "âŒ Failed to start server!" << std::endl;
            return 1;
        }
        
        std::cout << "ðŸŽ‰ AEON AI Platform is online!" << std::endl;
        std::cout << "ðŸŒ Access dashboard at: http://" << host << ":" << port << "/" << std::endl;
        std::cout << "ðŸ“Š View API documentation at: http://" << host << ":" << port << "/api" << std::endl;
        std::cout << "ðŸ’¡ Press Ctrl+C to shutdown server" << std::endl;
        std::cout << "\n" << std::string(60, '=') << std::endl;
        std::cout << "AEON AI SERVER LOG:" << std::endl;
        std::cout << std::string(60, '=') << std::endl;
        
        // Keep the main thread alive and show periodic stats
        int counter = 0;
        while (true) {
            std::this_thread::sleep_for(std::chrono::seconds(1));
            
            // Print periodic stats every 2 minutes
            if (++counter % 120 == 0 && g_server->getTotalRequests() > 0) {
                std::cout << "ðŸ“Š AEON AI Status | "
                          << "Requests: " << g_server->getTotalRequests() 
                          << " | Success: " << g_server->getSuccessfulGenerations()
                          << " | Errors: " << g_server->getFailedGenerations()
                          << " | Uptime: " << g_server->getUptime() << std::endl;
            }
        }
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ Server error: " << e.what() << std::endl;
        return 1;
    } catch (...) {
        std::cerr << "âŒ Unknown server error occurred!" << std::endl;
        return 1;
    }
    
    return 0;
}